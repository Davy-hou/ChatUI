{
  "cells": [
    {
      "cell_type": "raw",
      "id": "afaf8039",
      "metadata": {
        "id": "afaf8039"
      },
      "source": [
        "---\n",
        "sidebar_label: DeepSeek\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49f1e0d",
      "metadata": {
        "id": "e49f1e0d"
      },
      "source": [
        "# ChatDeepSeek\n",
        "\n",
        "\n",
        "This will help you get started with DeepSeek's hosted [chat models](/docs/concepts/chat_models). For detailed documentation of all ChatDeepSeek features and configurations head to the [API reference](https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html).\n",
        "\n",
        ":::tip\n",
        "\n",
        "DeepSeek's models are open source and can be run locally (e.g. in [Ollama](./ollama.ipynb)) or on other inference providers (e.g. [Fireworks](./fireworks.ipynb), [Together](./together.ipynb)) as well.\n",
        "\n",
        ":::\n",
        "\n",
        "## Overview\n",
        "### Integration details\n",
        "\n",
        "| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/deepseek) | Package downloads | Package latest |\n",
        "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
        "| [ChatDeepSeek](https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html) | [langchain-deepseek](https://python.langchain.com/api_reference/deepseek/) | ❌ | beta | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-deepseek?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-deepseek?style=flat-square&label=%20) |\n",
        "\n",
        "### Model features\n",
        "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | Native async | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
        "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |\n",
        "| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |\n",
        "\n",
        ":::note\n",
        "\n",
        "DeepSeek-R1, specified via `model=\"deepseek-reasoner\"`, does not support tool calling or structured output. Those features [are supported](https://api-docs.deepseek.com/guides/function_calling) by DeepSeek-V3 (specified via `model=\"deepseek-chat\"`).\n",
        "\n",
        ":::\n",
        "\n",
        "## Setup\n",
        "\n",
        "To access DeepSeek models you'll need to create a/an DeepSeek account, get an API key, and install the `langchain-deepseek` integration package.\n",
        "\n",
        "### Credentials\n",
        "\n",
        "Head to [DeepSeek's API Key page](https://platform.deepseek.com/api_keys) to sign up to DeepSeek and generate an API key. Once you've done this set the `DEEPSEEK_API_KEY` environment variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "433e8d2b-9519-4b49-b2c4-7ab65b046c94",
      "metadata": {
        "id": "433e8d2b-9519-4b49-b2c4-7ab65b046c94"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"DEEPSEEK_API_KEY\"):\n",
        "    os.environ[\"DEEPSEEK_API_KEY\"] = 'sk-e157ca06a73547a390f2b3794fbf805d'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ee0c4b-9764-423a-9dbf-95129e185210",
      "metadata": {
        "id": "72ee0c4b-9764-423a-9dbf-95129e185210"
      },
      "source": [
        "To enable automated tracing of your model calls, set your [LangSmith](https://docs.smith.langchain.com/) API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15d341e-3e26-4ca3-830b-5aab30ed66de",
      "metadata": {
        "id": "a15d341e-3e26-4ca3-830b-5aab30ed66de"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
      "metadata": {
        "id": "0730d6a1-c893-4840-9817-5e5251676d5d"
      },
      "source": [
        "### Installation\n",
        "\n",
        "The LangChain DeepSeek integration lives in the `langchain-deepseek` package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
        "outputId": "fa3e9154-b68d-49c6-bf0a-599e9ffff79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-deepseek"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38cde65-254d-4219-a441-068766c0d4b5",
      "metadata": {
        "id": "a38cde65-254d-4219-a441-068766c0d4b5"
      },
      "source": [
        "## Instantiation\n",
        "\n",
        "Now we can instantiate our model object and generate chat completions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
      "metadata": {
        "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae"
      },
      "outputs": [],
      "source": [
        "from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "llm = ChatDeepSeek(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    api_key='sk-e157ca06a73547a390f2b3794fbf805d',\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4f3e15",
      "metadata": {
        "id": "2b4f3e15"
      },
      "source": [
        "## Invocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "62e0dbc3",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62e0dbc3",
        "outputId": "081d555b-15f1-49a8-e0c0-ac3f20a6ee40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='La traduction de \"I love programming\" en français est :\\n\\n**J\\'aime la programmation.**\\n\\n### Explications :\\n- **J\\'aime** : Traduction de \"I love\" (verbe *aimer* conjugué à la première personne).\\n- **la programmation** : Correspond à \"programming\" dans le contexte informatique. En français, ce terme désigne spécifiquement l\\'activité de création de programmes informatiques.\\n\\n### Alternative courante :\\nSi le contexte est informel ou général (ex. plaisir de coder), on peut aussi dire :  \\n**J\\'adore coder.** (*Coder* étant un terme familier pour *programmer*).', additional_kwargs={'refusal': None, 'reasoning_content': 'We are translating from English to French.\\n The sentence is: \"I love programming.\"\\n In French:\\n   \"I\" -> \"Je\"\\n   \"love\" -> \"aime\" (first person singular of the verb \"aimer\")\\n   \"programming\" -> \"la programmation\" (as in the activity of writing computer programs)\\n\\n So the translation is: \"J\\'aime la programmation.\"\\n\\n Note: In French, we use an apostrophe for \"J\\'aime\" because \"Je\" ends with a vowel and \"aime\" begins with a vowel.\\n Also, \"programming\" is translated as \"la programmation\" to refer to the activity. However, note that in some contexts, especially in computing, \"programmation\" is used.\\n\\n But wait, there\\'s an alternative: sometimes \"programmer\" is used as a verb, but here the word is a noun (gerund) and the verb is \"love\".\\n Therefore, we translate \"programming\" as the noun \"la programmation\".\\n\\n So the complete translation: \"J\\'aime la programmation.\"'}, response_metadata={'token_usage': {'completion_tokens': 367, 'prompt_tokens': 25, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 221, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 25}, 'model_name': 'deepseek-reasoner', 'system_fingerprint': 'fp_393bca965e_prod0623_fp8_kvcache', 'id': '6f6c40f1-eedd-49c0-b875-fbeffcfb6671', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--66599df5-d440-4cb9-9d0d-6b9c1864ff84-0', usage_metadata={'input_tokens': 25, 'output_tokens': 367, 'total_tokens': 392, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 221}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
      "metadata": {
        "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8"
      },
      "source": [
        "## Chaining\n",
        "\n",
        "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"\"\"\n",
        "You are an expert at breaking down search queries into focused sub-queries to improve retrieval.\n",
        "Your task is to decompose the following query into 2-4 focused sub-queries that will help retrieve\n",
        "comprehensive information to answer the original query.\n",
        "\n",
        "Original query: what is ai\n",
        "\n",
        "For any query, create sub-queries that:\n",
        "1. Focus on different aspects or components of the original query\n",
        "2. Are self-contained and answerable independently\n",
        "3. Use specific terminology and keywords for better retrieval\n",
        "4. Cover the breadth of information needed to answer the original query\n",
        "\n",
        "Even for simple queries, create variations that:\n",
        "- Use different keywords or synonyms\n",
        "- Focus on specific aspects of the topic\n",
        "- Ask for different types of information (definitions, examples, explanations, etc.)\n",
        "\n",
        "Return your sub-queries as a JSON list with the following format:\n",
        "{{\n",
        "  \"sub_queries\": [\n",
        "    \"First focused sub-query here\",\n",
        "    \"Second focused sub-query here\",\n",
        "    \"Third focused sub-query here\"\n",
        "  ]}}\n",
        "\"\"\"\n",
        "response_schema = {\n",
        "  \"title\": \"QueryBreakdown\",\n",
        "  \"description\": \"Breakdown of query into focused sub-queries\",\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "      \"sub_queries\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\"type\": \"string\"},\n",
        "          \"description\": \"List of focused sub-queries to search for\"\n",
        "      }\n",
        "  },\n",
        "  \"required\": [\"sub_queries\", \"reasoning\"]\n",
        "}\n",
        "structured_llm = llm.with_structured_output(response_schema,method='json_mode')\n",
        "# Add instructions to the prompt to return the output in JSON format\n",
        "structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1plIrrmDVo4W",
        "outputId": "4fcf4628-5fc2-4c16-f0e7-bfe80c848f6f"
      },
      "id": "1plIrrmDVo4W",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sub_queries': ['Define artificial intelligence and its core principles',\n",
              "  'Provide examples of artificial intelligence applications in daily life',\n",
              "  'Explain the key technologies in artificial intelligence such as machine learning and neural networks']}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GetWeather(BaseModel):\n",
        "    '''Get the current weather in a given location'''\n",
        "\n",
        "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
        "\n",
        "class GetPopulation(BaseModel):\n",
        "    '''Get the current population in a given location'''\n",
        "\n",
        "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
        "\n",
        "llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
        "ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n",
        "ai_msg.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUYNadiIBPOl",
        "outputId": "11ef29b7-83e2-436f-9219-ff899d1ae418"
      },
      "id": "oUYNadiIBPOl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'GetWeather',\n",
              "  'args': {'location': 'Los Angeles, CA'},\n",
              "  'id': 'call_0_c10a54e2-00a6-4d12-824d-5ee3eb0449d9',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'GetWeather',\n",
              "  'args': {'location': 'New York, NY'},\n",
              "  'id': 'call_1_da99a803-65cf-4bac-a831-c7d549460e8c',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
      "metadata": {
        "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
      "metadata": {
        "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3"
      },
      "source": [
        "## API reference\n",
        "\n",
        "For detailed documentation of all ChatDeepSeek features and configurations head to the [API Reference](https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}